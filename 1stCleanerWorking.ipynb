{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No new Excel files to process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# Input and output\n",
    "input_folder = \"ExcelFolders\"\n",
    "output_file = os.path.join(\"cleanExcel\", \"cleanedBook.xlsx\")\n",
    "log_file = \"log.txt\"\n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(\"cleanExcel\", exist_ok=True)\n",
    "\n",
    "# Load log of already processed files\n",
    "if os.path.exists(log_file):\n",
    "    with open(log_file, \"r\") as f:\n",
    "        processed_files = set(f.read().splitlines())\n",
    "else:\n",
    "    processed_files = set()\n",
    "\n",
    "# Collect all cleaned DataFrames\n",
    "cleaned_dfs = []\n",
    "\n",
    "# Loop through all Excel files in input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".xlsx\") and filename not in processed_files:\n",
    "        filepath = os.path.join(input_folder, filename)\n",
    "\n",
    "        print(f\"Processing: {filename}\")\n",
    "\n",
    "        # 1. Load workbook and unmerge cells\n",
    "        wb = load_workbook(filepath)\n",
    "        ws = wb.active\n",
    "        for merged_range in list(ws.merged_cells.ranges):\n",
    "            ws.unmerge_cells(str(merged_range))\n",
    "        wb.save(filepath)\n",
    "\n",
    "        # 2. Read Excel into DataFrame\n",
    "        df = pd.read_excel(filepath, header=None)\n",
    "\n",
    "        # Drop completely empty rows\n",
    "        df = df.dropna(how=\"all\")\n",
    "\n",
    "        # Drop the first column (index 0)\n",
    "        if df.shape[1] > 0:\n",
    "            df = df.drop(df.columns[0], axis=1)\n",
    "        df.insert(0, \"EmptyCol1\", \"\")\n",
    "        df.insert(0, \"EmptyCol2\", \"\")\n",
    "\n",
    "        print(\"Number of columns in df:\", df.shape[1])\n",
    "        print(\"First few rows:\")\n",
    "        print(df.head(5))\n",
    "\n",
    "        cleaned_dfs.append(df)\n",
    "\n",
    "        # Add to log file after successful processing\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(filename + \"\\n\")\n",
    "\n",
    "# Combine all cleaned DataFrames\n",
    "if cleaned_dfs:\n",
    "    final_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "    # Trim to 36 columns max\n",
    "    final_df = final_df.iloc[:, :36]\n",
    "    final_df.to_excel(output_file, index=False, header=False)\n",
    "    print(\"Final row count:\", final_df.shape[0])\n",
    "else:\n",
    "    print(\"⚠ No new Excel files to process.\")\n",
    "    sys.exit()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d249f920",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[34] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_df = \u001b[43mfinal_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m35\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m final_df.columns = [\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMonth_year\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mConsultation_Type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCase\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUnder 1 Male\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUnder 1 Female\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m70 Over Male\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m70 Over Female\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m ]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 4. Save back to cleanedBook.xlsx\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:7098\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7097\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7098\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7099\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: '[34] not found in axis'"
     ]
    }
   ],
   "source": [
    "final_df = final_df.drop(df.columns[35], axis=1)\n",
    "\n",
    "final_df.columns = [\n",
    "    \"Month_year\", \"Consultation_Type\", \"Case\",\n",
    "    \"Under 1 Male\", \"Under 1 Female\",\n",
    "    \"1-4 Male\", \"1-4 Female\",\n",
    "    \"5-9 Male\", \"5-9 Female\",\n",
    "    \"10-14 Male\", \"10-14 Female\",\n",
    "    \"15-18 Male\", \"15-18 Female\",\n",
    "    \"19-24 Male\", \"19-24 Female\",\n",
    "    \"25-29 Male\", \"25-29 Female\",\n",
    "    \"30-34 Male\", \"30-34 Female\",\n",
    "    \"35-39 Male\", \"35-39 Female\",\n",
    "    \"40-44 Male\", \"40-44 Female\",\n",
    "    \"45-49 Male\", \"45-49 Female\",\n",
    "    \"50-54 Male\", \"50-54 Female\",\n",
    "    \"55-59 Male\", \"55-59 Female\",\n",
    "    \"60-64 Male\", \"60-64 Female\",\n",
    "    \"65-69 Male\", \"65-69 Female\",\n",
    "    \"70 Over Male\", \"70 Over Female\"\n",
    "]\n",
    "# 4. Save back to cleanedBook.xlsx\n",
    "final_df.to_excel(\"cleanExcel/cleanedBook.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: AUGUST 2023 at row 3\n",
      "Found: AUGUST 2023 at row 22\n",
      "Found: AUGUST 2023 at row 41\n",
      "Found: DECEMBER 2023 at row 60\n",
      "Found: DECEMBER 2023 at row 79\n",
      "Found: DECEMBER 2023 at row 98\n",
      "Found: JULY 2023 at row 115\n",
      "Found: JULY 2023 at row 134\n",
      "Found: JULY 2023 at row 153\n",
      "Found: JUNE 2023 at row 172\n",
      "Found: JUNE 2023 at row 191\n",
      "Found: MAY 2023 at row 210\n",
      "Found: MAY 2023 at row 229\n",
      "Found: MAY 2023 at row 248\n",
      "Found: MAY 2023 at row 267\n",
      "Found: NOVEMBER 2023 at row 284\n",
      "Found: NOVEMBER 2023 at row 303\n",
      "Found: NOVEMBER 2023 at row 322\n",
      "Found: OCTOBER 2023 at row 341\n",
      "Found: OCTOBER 2023 at row 360\n",
      "Found: OCTOBER 2023 at row 379\n",
      "Found: SEPTEMBER 2023 at row 397\n",
      "Found: SEPTEMBER 2023 at row 416\n",
      "Found: SEPTEMBER 2023 at row 435\n",
      "✅ Updated file saved: cleanExcel\\cleanedBook.xlsx\n",
      "Number of rows: 450\n",
      "Number of columns: 35\n"
     ]
    }
   ],
   "source": [
    "month_year_map = {}\n",
    "current_month_year = None\n",
    "\n",
    "for i, row in final_df.iterrows():\n",
    "    for cell in row.dropna().astype(str):\n",
    "        if \"MONTH AND YEAR:\" in cell.upper():  # detect header rows\n",
    "            # Extract \"MONTH YEAR\"\n",
    "            match = re.search(r\"MONTH AND YEAR:\\s*([A-Z]+)\\s+(\\d{4})\", cell.upper())\n",
    "            if match:\n",
    "                current_month_year = \" \".join(cell.split(\":\")[1].strip().split()[:2])\n",
    "                print(f\"Found: {current_month_year} at row {i}\")\n",
    "    month_year_map[i] = current_month_year  # keep filling for all rows\n",
    "\n",
    "# ✅ Add/Update Month_year column\n",
    "if \"Month_year\" in final_df.columns:\n",
    "    final_df[\"Month_year\"] = pd.Series(month_year_map)\n",
    "else:\n",
    "    final_df.insert(0, \"Month_year\", pd.Series(month_year_map))  # put as first column\n",
    "\n",
    "# ✅ Save back to Excel\n",
    "output_file = os.path.join(\"cleanExcel\", \"cleanedBook.xlsx\")\n",
    "final_df.to_excel(output_file, index=False, header=True)\n",
    "\n",
    "print(f\"✅ Updated file saved: {output_file}\")\n",
    "print(\"Number of rows:\", final_df.shape[0])\n",
    "print(\"Number of columns:\", final_df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636f089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found: Consultation at row 4\n",
      "✅ Found: Diagnosis at row 23\n",
      "✅ Found: Mortality at row 42\n",
      "✅ Found: Consultation at row 61\n",
      "✅ Found: Diagnosis at row 80\n",
      "✅ Found: Mortality at row 99\n",
      "✅ Found: Consultation at row 116\n",
      "✅ Found: Diagnosis at row 135\n",
      "✅ Found: Mortality at row 154\n",
      "✅ Found: Consultation at row 173\n",
      "✅ Found: Diagnosis at row 192\n",
      "✅ Found: Mortality at row 211\n",
      "✅ Found: Consultation at row 230\n",
      "✅ Found: Consultation at row 249\n",
      "✅ Found: Consultation at row 268\n",
      "✅ Found: Consultation at row 285\n",
      "✅ Found: Diagnosis at row 304\n",
      "✅ Found: Mortality at row 323\n",
      "✅ Found: Consultation at row 342\n",
      "✅ Found: Diagnosis at row 361\n",
      "✅ Found: Mortality at row 380\n",
      "✅ Found: Consultation at row 398\n",
      "✅ Found: Diagnosis at row 417\n",
      "✅ Found: Mortality at row 436\n",
      "\n",
      "✅ File updated and saved to: cleanExcel\\cleanedBook.xlsx\n",
      "🔍 Unique categories found: {'Consultation', 'Diagnosis', 'Mortality'}\n"
     ]
    }
   ],
   "source": [
    "category_map = {}\n",
    "current_category = None\n",
    "found_categories = []\n",
    "\n",
    "for i, row in final_df.iterrows():\n",
    "    for cell in row.dropna().astype(str):\n",
    "        if \"TOP 10\" in cell.upper():\n",
    "            # Get the last word, strip punctuation\n",
    "            last_word = re.sub(r\"[^\\w]\", \"\", cell.strip().split()[-1])\n",
    "            current_category = last_word.capitalize()\n",
    "            found_categories.append((i, current_category))\n",
    "            print(f\"✅ Found: {current_category} at row {i}\")\n",
    "    category_map[i] = current_category  # Fill forward\n",
    "\n",
    "# ✅ Update existing Consultation_Type column\n",
    "final_df[\"Consultation_Type\"] = pd.Series(category_map)\n",
    "\n",
    "# ✅ Save back to Excel\n",
    "final_df.to_excel(output_file, index=False, header=True)\n",
    "\n",
    "print(f\"\\n✅ File updated and saved to: {output_file}\")\n",
    "print(\"🔍 Unique categories found:\", set(cat for _, cat in found_categories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cae85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Removed 211 rows and updated file: cleanExcel\\cleanedBook.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"cleanExcel\", \"cleanedBook.xlsx\")\n",
    "\n",
    "# Load Excel\n",
    "final_df = pd.read_excel(file_path)\n",
    "\n",
    "# Find and drop the row + 6 rows under it\n",
    "drop_indexes = []\n",
    "for i, row in final_df.iterrows():\n",
    "    for cell in row.dropna().astype(str):\n",
    "        if \"PASIG CITY CHILDREN'S HOSPITAL/PASIG CITY COVID-19 REFERRAL CENTER\" in cell.upper():\n",
    "            drop_indexes.extend(range(i, i + 8))  # this row + 6 below\n",
    "            break\n",
    "        \n",
    "# --- Remove rows containing \"TOTAL\" (disregard format) ---\n",
    "for i, row in final_df.iterrows():\n",
    "    for cell in row.dropna().astype(str):\n",
    "        if \"TOTAL\" in cell.upper().strip():\n",
    "            drop_indexes.append(i)\n",
    "            break\n",
    "\n",
    "# Drop them\n",
    "final_df = final_df.drop(drop_indexes, errors=\"ignore\").reset_index(drop=True)\n",
    "\n",
    "# ✅ Save back to the same Excel file\n",
    "final_df.to_excel(file_path, index=False, header=True)\n",
    "\n",
    "print(f\"✅ Removed {len(drop_indexes)} rows and updated file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba3b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLPASIG\\AppData\\Local\\Temp\\ipykernel_23692\\466000972.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  final_df = final_df.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Replace empty/NaN cells with 0\n",
    "final_df = final_df.fillna(0)\n",
    "\n",
    "# ✅ Save back to the same Excel file\n",
    "final_df.to_excel(file_path, index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c232068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
